{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\">Introduction</a></span></li><li><span><a href=\"#Loading-the-data\" data-toc-modified-id=\"Loading-the-data-2\">Loading the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-all-the-data-and-randomly-split-it-into-1500-training-examples-and-500-validation-examples.\" data-toc-modified-id=\"Load-all-the-data-and-randomly-split-it-into-1500-training-examples-and-500-validation-examples.-2.1\">Load all the data and randomly split it into 1500 training examples and 500 validation examples.</a></span></li></ul></li><li><span><a href=\"#Sparse-Representations\" data-toc-modified-id=\"Sparse-Representations-3\">Sparse Representations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Function-that-converts-an-example-(e.g.-a-list-of-words)-into-a-sparse-bag-of-words-representation:\" data-toc-modified-id=\"Function-that-converts-an-example-(e.g.-a-list-of-words)-into-a-sparse-bag-of-words-representation:-3.1\">Function that converts an example (e.g. a list of words) into a sparse bag-of-words representation:</a></span></li></ul></li><li><span><a href=\"#Support-Vector-Machine-via-Pegasos\" data-toc-modified-id=\"Support-Vector-Machine-via-Pegasos-4\">Support Vector Machine via Pegasos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pegasos-algorithm\" data-toc-modified-id=\"Pegasos-algorithm-4.1\">Pegasos algorithm</a></span></li><li><span><a href=\"#Support-Vector-Machine-(SVM)\" data-toc-modified-id=\"Support-Vector-Machine-(SVM)-4.2\">Support Vector Machine (SVM)</a></span></li><li><span><a href=\"#Implementation-of-the-Pegasos-algorithm-to-run-on-a-sparse-data-representation\" data-toc-modified-id=\"Implementation-of-the-Pegasos-algorithm-to-run-on-a-sparse-data-representation-4.3\">Implementation of the Pegasos algorithm to run on a sparse data representation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Version-1\" data-toc-modified-id=\"Version-1-4.3.1\">Version 1</a></span></li><li><span><a href=\"#Version-2\" data-toc-modified-id=\"Version-2-4.3.2\">Version 2</a></span></li></ul></li><li><span><a href=\"#Comparison-of-the-two-approaches\" data-toc-modified-id=\"Comparison-of-the-two-approaches-4.4\">Comparison of the two approaches</a></span></li><li><span><a href=\"#Loss-function\" data-toc-modified-id=\"Loss-function-4.5\">Loss function</a></span></li><li><span><a href=\"#Regularization-parameter-optimization\" data-toc-modified-id=\"Regularization-parameter-optimization-4.6\">Regularization parameter optimization</a></span></li><li><span><a href=\"#Score\" data-toc-modified-id=\"Score-4.7\">Score</a></span></li><li><span><a href=\"#Differentiability\" data-toc-modified-id=\"Differentiability-4.8\">Differentiability</a></span></li></ul></li><li><span><a href=\"#Error-analysis\" data-toc-modified-id=\"Error-analysis-5\">Error analysis</a></span></li><li><span><a href=\"#Features\" data-toc-modified-id=\"Features-6\">Features</a></span><ul class=\"toc-item\"><li><span><a href=\"#Stopwords\" data-toc-modified-id=\"Stopwords-6.1\">Stopwords</a></span></li><li><span><a href=\"#Loss-function-with-standard-error\" data-toc-modified-id=\"Loss-function-with-standard-error-6.2\">Loss function with standard error</a></span></li><li><span><a href=\"#Other-ideas-for-new-features:\" data-toc-modified-id=\"Other-ideas-for-new-features:-6.3\">Other ideas for new features:</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: SVM and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment for the [Foundations of Machine Learning](https://bloomberg.github.io/foml/#about) course.\n",
    "The assignment text can be found [here]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Goal of the assignment: perform sentiment analysis on movie reviews. We will be working with **natural language data**. \n",
    "- First, the **Pegasos algorithm** is implemented. Pegasos is essentially stochastic subgradient descent for the SVM with a particular schedule for the step-size.\n",
    "- Second, because in natural language domains we typically have huge feature spaces, we work with **sparse representations** of feature vectors, where only the non-zero entries are explicitly recorded. This will require coding the gradient and SGD code using hash tables (dictionaries in Python), rather than numpy arrays.\n",
    "- Then we will perform feature engineering to improve our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will be using the [Polarity Dataset v2.0](https://www.cs.cornell.edu/people/pabo/movie-review-data/), constructed by Pang and Lee. \n",
    "- It has the full text from 2000 movies reviews: 1000 reviews are classified as “positive” and 1000 as “negative.” - Our goal is to predict whether a review has positive or negative sentiment from the text of the review. \n",
    "- Each review is stored in a separate file: the positive reviews are in a folder called “pos”, and the negative reviews are in “neg”. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all the data and randomly split it into 1500 training examples and 500 validation examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################Code provided in the assignment###################################################\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "'''\n",
    "Note:  This code is just a hint for people who are not familiar with text processing in python. \n",
    "There is no obligation to use this code, though you may if you like. \n",
    "'''\n",
    "\n",
    "\n",
    "def folder_list(path,label):\n",
    "    '''\n",
    "    PARAMETER PATH IS THE PATH OF YOUR LOCAL FOLDER\n",
    "    '''\n",
    "    filelist = os.listdir(path)\n",
    "    review = []\n",
    "    for infile in filelist:\n",
    "        file = os.path.join(path,infile)\n",
    "        r = read_data(file)\n",
    "        r.append(label)\n",
    "        review.append(r)\n",
    "    return review\n",
    "\n",
    "def read_data(file):\n",
    "    '''\n",
    "    Read each file into a list of strings. \n",
    "    Example:\n",
    "    [\"it's\", 'a', 'curious', 'thing', \"i've\", 'found', 'that', 'when', 'willis', 'is', 'not', 'called', 'on', \n",
    "    ...'to', 'carry', 'the', 'whole', 'movie', \"he's\", 'much', 'better', 'and', 'so', 'is', 'the', 'movie']\n",
    "    '''\n",
    "    f = open(file)\n",
    "    lines = f.read().split(' ') #split each line on whitespace\n",
    "    symbols = '${}()[].,:;+-*/&|<>=~\" '\n",
    "    words = map(lambda Element: Element.translate(str.maketrans(\"\", \"\", symbols)).strip(), lines)\n",
    "    words = filter(None, words)\n",
    "    list_words = list(words)\n",
    "    return list_words\n",
    "\n",
    "###############################################\n",
    "######## YOUR CODE STARTS FROM HERE. ##########\n",
    "###############################################\n",
    "\n",
    "def shuffle_data():\n",
    "    '''\n",
    "    pos_path is where you save positive review data.\n",
    "    neg_path is where you save negative review data.\n",
    "    '''\n",
    "    pos_path = '/Users/Coline/Desktop/mlprojects/Bloomberg_foundations_of_ML/hw3-svm/data/pos'\n",
    "    neg_path = '/Users/Coline/Desktop/mlprojects/Bloomberg_foundations_of_ML/hw3-svm/data/neg'\n",
    "\n",
    "    #pos_path = r\"H:\\Ny mapp\\Literature\\Bayesian prob\\Bloomsberg lectures\\Week8\\hw3\\data\\data\\pos\"\n",
    "    #neg_path = r\"H:\\Ny mapp\\Literature\\Bayesian prob\\Bloomsberg lectures\\Week8\\hw3\\data\\data\\neg\"\n",
    "    \n",
    "    pos_review = folder_list(pos_path,1)\n",
    "    neg_review = folder_list(neg_path,-1)\n",
    "\n",
    "    review = pos_review + neg_review\n",
    "    random.shuffle(review)\n",
    "  \n",
    "    '''\n",
    "    Now you have read all the files into list 'review' and it has been shuffled.\n",
    "    Save your shuffled result by pickle.\n",
    "    *Pickle is a useful module to serialize a python object structure. \n",
    "    *Check it out. https://wiki.python.org/moin/UsingPickle\n",
    "    '''\n",
    "    train = review[:1500]\n",
    "    valid = review[1500:]\n",
    "    \n",
    "    with open('train.pkl', 'wb') as f:\n",
    "        pickle.dump(train, f)\n",
    "    \n",
    "    with open('valid.pkl', 'wb') as f:\n",
    "        pickle.dump(valid, f)\n",
    "    \n",
    "    return train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = shuffle_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load files from memory\n",
    "with open('train.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open('valid.pkl', 'rb') as f:\n",
    "    valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The most basic way to represent text documents for machine learning is with a **“bag-of-words”** representation. \n",
    "- Here every possible word is a feature, and the value of a word feature is the number of times that word appears in the document. Of course, most words will not appear in any particular document, and those counts will be zero. \n",
    "- Rather than store a huge number of zeros, we use a **sparse representation**, in which we only store the counts that are nonzero. The counts are stored in a key/value store (such as a dictionary in Python).  \n",
    "For example, “Harry Potter and Harry Potter II” would be represented as the following Python dict: ``x={’Harry’:2, ’Potter’:2, ’and’:1, ’II’:1}``.  \n",
    "- We will be using linear classifiers of the form $f(x) = w^T x$, and we can store the $w$ vector in a sparse format as well, such as ``w={’minimal’:1.3, ’Harry’:-1.1, ’viable’:-4.2, ’and’:2.2, ’product’:9.1}``. \n",
    "- The inner product between $w$ and $x$ would only involve the features that appear in both $x$ and $w$, since whatever doesn’t appear is assumed to be zero. For this example, the inner product would be ``x[Harry] * w[Harry] + x[and] * w[and] = 2*(-1.1) + 1*(2.2)``.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that converts an example (e.g. a list of words) into a sparse bag-of-words representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def bag_of_words(review):\n",
    "    '''\n",
    "    Converts a list of words into a sparse bag-of-words representation.\n",
    "    @Param review: list (iterable)\n",
    "    Returns a dictionary with keys=words and values=number of times the words appear\n",
    "    '''\n",
    "    cnt = Counter()\n",
    "    for word in review:\n",
    "        cnt[word] += 1\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine via Pegasos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question you will build an SVM using the Pegasos algorithm. To align with the notation used in the [Pegasos paper](https://ttic.uchicago.edu/~nati/Publications/PegasosMPB.pdf), we’re considering the following formulation of the SVM objective function: $$ min_{w \\in R^d} \\frac{\\lambda}{2} + \\frac{1}{m} \\sum_{i=1}^m max\\{0, 1-y_iw^Tx_i\\} $$\n",
    "\n",
    "Note that, for simplicity, we are leaving off the unregularized bias term $b$. \n",
    "\n",
    "### Pegasos algorithm\n",
    "Pegasos is stochastic subgradient descent using a step size rule $\\eta_t = \\frac{1}{\\lambda t}$. The pseudocode is given below:\n",
    "\n",
    "***\n",
    "Input: $\\lambda > 0$. Choose $w_1 = 0; t = 0$\n",
    "\n",
    "While termination condition not met  \n",
    "\n",
    "&nbsp;&nbsp;For $j = 1,...,m$ (assumes data is randomly permuted)  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$t = t + 1$  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$\\eta_t = \\frac{1}{t\\lambda}$;  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;If $y_j w^T_t x_j < 1$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$w_{t+1} = (1 - \\eta_t \\lambda)w_t + \\eta_t y_j x_j$  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Else  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$w_{t+1} = (1 - \\eta_t \\lambda)w_t$ \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "The “stochastic” SVM objective function is the SVM objective function with a single training point:  \n",
    "  \n",
    "$$J_i(w) = \\frac{\\lambda}{2} \\|w\\|^2 + max\\{0, 1-y_iw^Tx_i\\}$$   \n",
    "The function  $J_i(w)$ is not differentiable at $y_i w^T x_i = 1$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subgradient of $J_i(w)$ is given by \n",
    "\n",
    "$$ g =\n",
    "  \\begin{cases}\n",
    "    \\lambda w - y_i x_i       & \\quad \\text{for } y_i w^T x_i < 1 \\\\\n",
    "    \\lambda w  & \\quad \\text{for } y_i w^T x_i \\geq 1\n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the step size rule is $\\eta_t = \\frac{1}{\\lambda t}$, then doing SGD with the subgradient direction from the previous problem is the same as given in the pseudocode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the Pegasos algorithm to run on a sparse data representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################code given in the assignment#############################################\n",
    "# Taken from http://web.stanford.edu/class/cs221/ Assignment #2 Support Code\n",
    "\n",
    "def dotProduct(d1, d2):\n",
    "    \"\"\"\n",
    "    @param dict d1: a feature vector represented by a mapping from a feature (string) to a weight (float).\n",
    "    @param dict d2: same as d1\n",
    "    @return float: the dot product between d1 and d2\n",
    "    \"\"\"\n",
    "    if len(d1) < len(d2):\n",
    "        return dotProduct(d2, d1)\n",
    "    else:\n",
    "        return sum(d1.get(f, 0) * v for f, v in d2.items())\n",
    "\n",
    "def increment(d1, scale, d2):\n",
    "    \"\"\"\n",
    "    Implements d1 += scale * d2 for sparse vectors.\n",
    "    @param dict d1: the feature vector which is mutated.\n",
    "    @param float scale\n",
    "    @param dict d2: a feature vector.\n",
    "\n",
    "    NOTE: This function does not return anything, but rather\n",
    "    increments d1 in place. We do this because it is much faster to\n",
    "    change elements of d1 in place than to build a new dictionary and\n",
    "    return it.\n",
    "    \"\"\"\n",
    "    for f, v in d2.items():\n",
    "        d1[f] = d1.get(f, 0) + v * scale\n",
    "        \n",
    "#########################################end of code given in the assignment#####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pegasos(train, epoch, l_reg=0.1):\n",
    "    '''\n",
    "    Implements Pegasos algorithm running on sparse data representation. \n",
    "    @param train: list of lists, each list contains single words from the review\n",
    "    @param epoch: number of epochs, integer\n",
    "    @param l_reg: regularization parameter lambda, float\n",
    "    \n",
    "    The weight dictionnary w is initialized as an empty dictionnary. \n",
    "    For each epoch, the algorithm passes through each datapoint of the randomized dataset and the step size \n",
    "    is decreased by a factor of 1/t (t increments after each datapoint).\n",
    "    Each datapoint is transformed into a bag of words using the bag_of_words function, and the label y_j is \n",
    "    extracted. The gradient is calculated using the appropriate subgradient, defined by the value of the\n",
    "    margin (calculated using the dotProduct function). The weight dictionnary is updated with the gradient\n",
    "    and step size using the increment function.\n",
    "    '''\n",
    "    w = {}\n",
    "    t = 0\n",
    "\n",
    "    for i in range(epoch):\n",
    "        \n",
    "        if i%10 == 0:\n",
    "            print(\"Epoch: {}\".format(i))\n",
    "        \n",
    "        for j in range(len(train)):\n",
    "            t = t+1\n",
    "            step = 1/(l_reg*t)\n",
    "            x_j = bag_of_words(train[j][:-1])\n",
    "            y_j = train[j][-1]                   #The label is the last entry in the list\n",
    "            margin = y_j*dotProduct(w,x_j)\n",
    "        \n",
    "            if margin < 1:\n",
    "                increment(w, -step*l_reg, w)\n",
    "                increment(w, step*y_j, x_j)\n",
    "            \n",
    "            else:\n",
    "                increment(w, -step*l_reg, w)\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    }
   ],
   "source": [
    "w1 = pegasos(train, epoch=2, l_reg=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that in every step of the Pegasos algorithm, we rescale every entry of $w_t$ by the factor $(1 − \\eta_t \\lambda)$. \n",
    "- Implementing this directly with dictionaries is very slow. \n",
    "- We can make things significantly faster by representing $w$ as $w = sW$, where $s \\in R$ and $W \\in R^d$. \n",
    "- You can start with $s = 1$ and $W$ all zeros (i.e. an empty dictionary). Note that both updates (i.e. whether or not we have a margin error) start with rescaling $w_t$, which we can do simply by setting $s_{t+1} = (1 − \\eta_t \\lambda)s_t$. If the update is $w_{t+1} = (1 − \\eta_t \\lambda)w_t + \\eta_t y_j x_j$, then **verify that the Pegasos update step is equivalent to**:  \n",
    "\n",
    "$$s_{t+1} = (1 − \\eta_t \\lambda)s_t$$\n",
    "$$W_{t+1} = W_t + \\frac{1}{s_{t+1}} \\eta_t y_j x_j.$$\n",
    "\n",
    "There is one subtle issue with the approach described above: if we ever have $1 − \\eta_t \\lambda = 0$, then $s_{t+1} = 0$, and we’ll have a divide by 0 in the calculation for $W_{t+1}$. This only happens when $\\eta_t = 1/\\lambda$. With our step-size rule of $\\eta_t = 1/\\lambda t$, it happens exactly when $t = 1$. So one approach is to just start at $t = 2$. More generically, note that if $s_{t+1} = 0$, then $w_{t+1} = 0$. Thus an equivalent representation is $s_{t+1} = 1$ and $W = 0$. Thus if we ever get $s_{t+1} = 0$, simply set it back to 1 and reset $W_{t+1}$ to zero, which is an empty dictionary in a sparse representation.  \n",
    "Implement the Pegasos algorithm with the $(s, W)$ representation described above. [See section 5.1 of Leon Bottou’s Stochastic Gradient Tricks for a more generic version of this technique, and many other useful tricks.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pegasos_v2(train, valid, epoch, l_reg=0.1, tol=1e-9):\n",
    "    '''\n",
    "    pegasos_v2 function implements Pegasos algorithm running on sparse data representation. \n",
    "    @param train: list of lists, each list contains single words from the review. Training set\n",
    "    @param valid: list of lists, each list contains single words from the review. Validation set\n",
    "    @param epoch: number of epochs, integer\n",
    "    @param l_reg: regularization parameter lambda, float\n",
    "    @param tol: tolerance level for estimation of convergence, float\n",
    "    \n",
    "    The weight dictionnary w is represented as w=sW, where s in is R and W is in Rd. \n",
    "    The parameters initialization is s=1 and W empty dictionary.\n",
    "    '''\n",
    "    s = 1\n",
    "    W = {}\n",
    "    t = 1\n",
    "    w = {}\n",
    "\n",
    "    for i in range(epoch):\n",
    "         \n",
    "        #Storing the weight vector to perform estimation of convergence after each epoch\n",
    "        w_old = W.copy() \n",
    "        for a,b in w_old.items():\n",
    "            w_old[a] = b*s_new\n",
    "\n",
    "        for j in range(len(train)):\n",
    "            t = t+1\n",
    "            step = 1/(l_reg*t)\n",
    "            x_j = bag_of_words(train[j][:-1])\n",
    "            y_j = train[j][-1]\n",
    "             \n",
    "            margin = s*y_j*dotProduct(W,x_j)\n",
    "            \n",
    "            if margin < 1:\n",
    "                s_new = s*(1-step*l_reg)\n",
    "                increment(W, (step*y_j)/s_new, x_j)\n",
    "                s = s_new\n",
    "            \n",
    "            else:\n",
    "                s_new = s*(1-step*l_reg)\n",
    "                s = s_new    \n",
    "            \n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print(\"Epoch: {}\".format(i))\n",
    "            w = W.copy()\n",
    "            for a,b in w.items():\n",
    "                w[a] = b*s_new\n",
    "            loss_val = loss(valid,w)\n",
    "            print('The model classified {}% of the examples correctly'.format(loss_val))\n",
    "        \n",
    "        #Testing for convergence:\n",
    "        if i != 0: \n",
    "            list_weights_diff = []\n",
    "            w = W.copy()\n",
    "            for a,b in w.items():\n",
    "                w[a] = b*s_new\n",
    "            for k, v in w_old.items():\n",
    "                list_weights_diff.append(np.abs(w.get(k, 0) - w_old[k])) \n",
    "        \n",
    "            if np.all(np.asarray(list_weights_diff) < tol):\n",
    "                print('converged')\n",
    "                break   \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the two approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "CPU times: user 23.3 s, sys: 163 ms, total: 23.4 s\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%time w1 = pegasos(train, epoch=2, l_reg=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "The model classified 63.6% of the examples correctly\n",
      "CPU times: user 1.61 s, sys: 15.4 ms, total: 1.63 s\n",
      "Wall time: 1.63 s\n"
     ]
    }
   ],
   "source": [
    "%time w2 = pegasos_v2(train, valid, epoch=2, l_reg=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(valid, w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(valid, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at the content of the weight dicts:\n",
    "list_weights_diff = []\n",
    "for k, v in w1.items():\n",
    "    list_weights_diff.append(np.abs(w2.get(k, 0) - w1[k]))\n",
    "np.all(np.asarray(list_weights_diff) < 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When expressing the vector $w_t$ as the product $s_tW_t$, where s is a scalar, as was done in pegasos_v2 algorithm, the algorithm is sped up by a factor of 14, which is significantly faster.\n",
    "- The performance of the algorithms was equivalent: they had the same classification error on the training set, and the weight vectors were very similar.\n",
    "- In the rest of this assignement, I will use pegasos_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(valid, w):\n",
    "    '''\n",
    "    function that takes a sparse weight vector w and a collection of (x,y) pairs,\n",
    "    and returns the percent error when predicting y using sign(w^Tx). \n",
    "    In other words, the function reports the 0-1 loss of the linear predictor x -> w^Tx.\n",
    "    '''\n",
    "    count = 0\n",
    "    for i in range(len(valid)):\n",
    "        y_i = valid[i][-1]\n",
    "        x_i = bag_of_words(valid[i][:-1])\n",
    "        pred = dotProduct(w,x_i)\n",
    "        \n",
    "        if (pred/y_i) > 0: #occurs when y_i and w^Tx_i have the same sign (ie correct prediction)\n",
    "            count += 1\n",
    "            \n",
    "    percent = (count/(len(valid)))*100\n",
    "    \n",
    "    return percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization parameter optimization\n",
    "\n",
    "- Using the bag-of-words feature representation described above, search for the regularization parameter that gives the minimal percent error on your test set. \n",
    "- Using the faster Pegasos implementation, running to convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "The model classified 51.0% of the examples correctly\n",
      "Epoch: 100\n",
      "The model classified 81.6% of the examples correctly\n",
      "Epoch: 200\n",
      "The model classified 81.6% of the examples correctly\n",
      "Epoch: 0\n",
      "The model classified 51.0% of the examples correctly\n",
      "Epoch: 100\n",
      "The model classified 81.6% of the examples correctly\n",
      "Epoch: 200\n",
      "The model classified 79.60000000000001% of the examples correctly\n",
      "Epoch: 0\n",
      "The model classified 51.0% of the examples correctly\n",
      "Epoch: 100\n",
      "The model classified 82.19999999999999% of the examples correctly\n",
      "Epoch: 200\n",
      "The model classified 82.19999999999999% of the examples correctly\n",
      "Epoch: 0\n",
      "The model classified 51.0% of the examples correctly\n",
      "Epoch: 100\n",
      "The model classified 80.2% of the examples correctly\n",
      "converged\n",
      "Epoch: 0\n",
      "The model classified 63.6% of the examples correctly\n",
      "converged\n",
      "Epoch: 0\n",
      "The model classified 69.19999999999999% of the examples correctly\n",
      "converged\n",
      "Epoch: 0\n",
      "The model classified 57.4% of the examples correctly\n",
      "converged\n",
      "Epoch: 0\n",
      "The model classified 63.0% of the examples correctly\n",
      "converged\n",
      "Epoch: 0\n",
      "The model classified 53.400000000000006% of the examples correctly\n",
      "converged\n",
      "Epoch: 0\n",
      "The model classified 56.8% of the examples correctly\n",
      "converged\n",
      "Epoch: 0\n",
      "The model classified 62.0% of the examples correctly\n",
      "converged\n",
      "Epoch: 0\n",
      "The model classified 56.599999999999994% of the examples correctly\n",
      "converged\n",
      "Epoch: 0\n",
      "The model classified 60.8% of the examples correctly\n",
      "converged\n"
     ]
    }
   ],
   "source": [
    "#Range of lambda values\n",
    "l_reg_arr = np.unique(np.concatenate((10.**np.arange(-6,1,1), np.arange(1,3,.3))))\n",
    "\n",
    "#Initialize values\n",
    "w_history = []\n",
    "loss_history = []\n",
    "\n",
    "#Run SVM algo for each lambda\n",
    "for l_reg in l_reg_arr:\n",
    "    w = pegasos_v2(train, valid, epoch=250, l_reg=l_reg, tol=1e-2)\n",
    "    w_history.append(w)\n",
    "    \n",
    "    #Calculate misclassification percentage for each lambda\n",
    "    loss_val = 100-loss(valid, w)\n",
    "    loss_history.append(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Param_l_reg  Classification error\n",
      "0      0.000001                  18.4\n",
      "1      0.000010                  18.6\n",
      "2      0.000100                  17.8\n",
      "3      0.001000                  18.4\n",
      "4      0.010000                  18.2\n",
      "5      0.100000                  18.0\n",
      "6      1.000000                  24.4\n",
      "7      1.300000                  23.4\n",
      "8      1.600000                  25.0\n",
      "9      1.900000                  24.6\n",
      "10     2.200000                  26.2\n",
      "11     2.500000                  25.6\n",
      "12     2.800000                  27.0\n",
      "The regression parameter that minimizes test loss was 1.00e-04.\n"
     ]
    }
   ],
   "source": [
    "#Present the results in a table\n",
    "import pandas as pd\n",
    "df_error = pd.DataFrame({'Param_l_reg': l_reg_arr, 'Classification error': loss_history})\n",
    "\n",
    "print(df_error)\n",
    "\n",
    "min_idx = df_error['Classification error'].idxmin()\n",
    "print('The regression parameter that minimizes test loss was {:.2e}.'.format(df_error['Param_l_reg'][min_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Classification error')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot validation performance vs regularization parameter\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.semilogx(df_error[\"Param_l_reg\"], df_error[\"Classification error\"])\n",
    "ax.grid()\n",
    "ax.set_title(\"Test Performance vs Regularization\")\n",
    "ax.set_xlabel(\"L-Penalty Regularization Parameter\")\n",
    "ax.set_ylabel(\"Classification error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, the regularization parameters in the range [1e-4,1e-2] achieved the best performance for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score is the value of the prediction $f(x) = w^Tx$  \n",
    "Does the magnitude of the score represent the confidence of the prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "The model classified 63.6% of the examples correctly\n",
      "converged\n"
     ]
    }
   ],
   "source": [
    "#Run pegasos_v2 with optimized regularization parameter\n",
    "w = pegasos_v2(train, valid, epoch=250, l_reg=1e-2, tol=1e-2)\n",
    "\n",
    "#Initiate arrays to store the score and error for each example\n",
    "score = np.zeros(len(valid))\n",
    "error = np.zeros(len(valid))\n",
    "\n",
    "for i in range(len(valid)):\n",
    "    y_i = valid[i][-1]\n",
    "    x_i = bag_of_words(valid[i][:-1])\n",
    "    score[i] = dotProduct(w,x_i)\n",
    "    if (score[i]/y_i) > 0:\n",
    "        error[i] = 0\n",
    "    else:\n",
    "        error[i] = 1 \n",
    "\n",
    "score_df = pd.DataFrame({'score':score, 'error': error})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>sum</th>\n",
       "      <th>error ratio (%)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(-10.318999999999999, -3.678]</th>\n",
       "      <td>50.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-3.678, -2.695]</th>\n",
       "      <td>50.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-2.695, -1.715]</th>\n",
       "      <td>50.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-1.715, -0.957]</th>\n",
       "      <td>50.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-0.957, -0.216]</th>\n",
       "      <td>50.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-0.216, 0.491]</th>\n",
       "      <td>50.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.491, 1.325]</th>\n",
       "      <td>50.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1.325, 2.175]</th>\n",
       "      <td>50.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2.175, 3.712]</th>\n",
       "      <td>50.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(3.712, 12.184]</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               count   sum  error ratio (%)\n",
       "score                                                      \n",
       "(-10.318999999999999, -3.678]   50.0   2.0              4.0\n",
       "(-3.678, -2.695]                50.0   4.0              8.0\n",
       "(-2.695, -1.715]                50.0   7.0             14.0\n",
       "(-1.715, -0.957]                50.0  15.0             30.0\n",
       "(-0.957, -0.216]                50.0  19.0             38.0\n",
       "(-0.216, 0.491]                 50.0  26.0             52.0\n",
       "(0.491, 1.325]                  50.0  10.0             20.0\n",
       "(1.325, 2.175]                  50.0   6.0             12.0\n",
       "(2.175, 3.712]                  50.0   2.0              4.0\n",
       "(3.712, 12.184]                 50.0   0.0              0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Divide the scores in groups of equal size\n",
    "quantiles = pd.qcut(score_df['score'], 10)\n",
    "\n",
    "#Function returning the count of elements in each group and the sum of errors\n",
    "def get_stats(group):\n",
    "    return {'count': group.count(), 'sum': group.sum()}\n",
    "\n",
    "#Group the dataset and apply the count and sum\n",
    "grouped = score_df['error'].groupby(quantiles)\n",
    "\n",
    "df = grouped.apply(get_stats).unstack()\n",
    "\n",
    "df['error ratio (%)'] = (df['sum']/df['count'])*100\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>sum</th>\n",
       "      <th>error ratio (%)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[-50, -2)</th>\n",
       "      <td>132.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.060606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[-2, 0)</th>\n",
       "      <td>132.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>35.606061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0, 1)</th>\n",
       "      <td>72.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>38.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[1, 2)</th>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[2, 50)</th>\n",
       "      <td>114.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.631579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count   sum  error ratio (%)\n",
       "score                                  \n",
       "[-50, -2)  132.0   8.0         6.060606\n",
       "[-2, 0)    132.0  47.0        35.606061\n",
       "[0, 1)      72.0  28.0        38.888889\n",
       "[1, 2)      50.0   5.0        10.000000\n",
       "[2, 50)    114.0   3.0         2.631579"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Divide the scores in groups of different size but more meaningful bins\n",
    "bins = [-50, -2,0,1,2, 50]\n",
    "cats = pd.cut(score_df['score'], bins, right=False)\n",
    "grouped2 = score_df['error'].groupby(cats)\n",
    "df2 = grouped2.apply(get_stats).unstack()\n",
    "df2['error ratio (%)'] = (df2['sum']/df2['count'])*100\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groups with small score amplitude (score in [-2,2)) have the highest error ratio. Higher magnitude scores lead to higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiability\n",
    "\n",
    "- The objective function is not differentiable when $y_iw^Tx_i = 1$. \n",
    "- How often and when do we have  $y_iw^Tx_i = 1$ (or perhaps within a small distance of 1)\n",
    "- If we didn’t know about subgradients, one might suggest just skipping the update when  $y_iw^Tx_i = 1$. Does this seem reasonable? What about shortening the step size by a small percentage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modified pegasos algo that counts the frequency of yiwTxi close to 1 \n",
    "\n",
    "def pegasos_v3(train, valid, epoch, l_reg=0.01, tol=1e-9):\n",
    "    '''\n",
    "    pegasos_v3 function implements Pegasos algorithm running on sparse data representation. \n",
    "    @param train: list of lists, each list contains single words from the review. Training set\n",
    "    @param valid: list of lists, each list contains single words from the review. Validation set\n",
    "    @param epoch: number of epochs, integer\n",
    "    @param l_reg: regularization parameter lambda, float\n",
    "    @param tol: tolerance level for estimation of convergence, float\n",
    "    \n",
    "    The weight dictionnary w is represented as w=sW, where s in is R and W is in Rd. \n",
    "    The parameters initialization is s=1 and W empty dictionary.\n",
    "    '''\n",
    "    s = 1\n",
    "    W = {}\n",
    "    t = 1\n",
    "    w = {}\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(epoch):\n",
    "         \n",
    "        #Storing the weight vector to perform estimation of convergence after each epoch\n",
    "        w_old = W.copy() \n",
    "        for a,b in w_old.items():\n",
    "            w_old[a] = b*s_new\n",
    "\n",
    "        for j in range(len(train)):\n",
    "            t = t+1\n",
    "            step = 1/(l_reg*t)\n",
    "            x_j = bag_of_words(train[j][:-1])\n",
    "            y_j = train[j][-1]\n",
    "             \n",
    "            margin = s*y_j*dotProduct(W,x_j)\n",
    "            \n",
    "            diff = np.abs(1-margin)\n",
    "            if diff < 1e-2:\n",
    "                cnt += 1\n",
    "            \n",
    "            if margin < 1:\n",
    "                s_new = s*(1-step*l_reg)\n",
    "                increment(W, (step*y_j)/s_new, x_j)\n",
    "                s = s_new\n",
    "            \n",
    "            else:\n",
    "                s_new = s*(1-step*l_reg)\n",
    "                s = s_new    \n",
    "            \n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print(\"Epoch: {}\".format(i))\n",
    "            w = W.copy()\n",
    "            for a,b in w.items():\n",
    "                w[a] = b*s_new\n",
    "            loss_val = loss(valid,w)\n",
    "            print('The model classified {}% of the examples correctly'.format(loss_val))\n",
    "        \n",
    "        #Testing for convergence:\n",
    "        if i != 0: \n",
    "            list_weights_diff = []\n",
    "            w = W.copy()\n",
    "            for a,b in w.items():\n",
    "                w[a] = b*s_new\n",
    "            for k, v in w_old.items():\n",
    "                list_weights_diff.append(np.abs(w.get(k, 0) - w_old[k])) \n",
    "        \n",
    "            if np.all(np.asarray(list_weights_diff) < tol):\n",
    "                print('converged')\n",
    "                break   \n",
    "    return w, i, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "The model classified 63.6% of the examples correctly\n",
      "Epoch: 100\n",
      "The model classified 82.0% of the examples correctly\n",
      "Epoch: 200\n",
      "The model classified 81.8% of the examples correctly\n"
     ]
    }
   ],
   "source": [
    "w, epoch, cnt = pegasos_v3(train, valid, epoch=300, l_reg=0.01, tol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 1976 occurences of the margin being close to 1 (tolerance 1e-2), while we cycled 300 times through 1500 training examples, which corresponds to a frequency of 0.44 %\n"
     ]
    }
   ],
   "source": [
    "frequency = (cnt / (epoch * len(train)))*100\n",
    "\n",
    "print(\"There were {0} occurences of the margin being close to 1 (tolerance 1e-2), while we cycled {1} times through {2} training examples, which corresponds to a frequency of {3:.2f} %\".format(cnt,epoch+1, len(train), frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis\n",
    "\n",
    "Why does the model get results wrong? Detailed analysis of two examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 14,\n",
       " 15,\n",
       " 19,\n",
       " 40,\n",
       " 46,\n",
       " 58,\n",
       " 65,\n",
       " 83,\n",
       " 97,\n",
       " 98,\n",
       " 106,\n",
       " 114,\n",
       " 122,\n",
       " 128,\n",
       " 129,\n",
       " 134,\n",
       " 137,\n",
       " 143]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make a list of indices corresponding to inputs that were predicted wrong:\n",
    "\n",
    "list_idx = []\n",
    "for i in range(len(valid)):\n",
    "    y_i = valid[i][-1]\n",
    "    x_i = bag_of_words(valid[i][:-1])\n",
    "    pred = dotProduct(w,x_i)\n",
    "    if pred/y_i < 0:\n",
    "        list_idx.append(i)\n",
    "list_idx[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>weight</th>\n",
       "      <th>feature_value</th>\n",
       "      <th>feature_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.421332</td>\n",
       "      <td>0.052667</td>\n",
       "      <td>8</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.351110</td>\n",
       "      <td>0.087778</td>\n",
       "      <td>4</td>\n",
       "      <td>see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.343999</td>\n",
       "      <td>0.038222</td>\n",
       "      <td>9</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.291555</td>\n",
       "      <td>0.036444</td>\n",
       "      <td>8</td>\n",
       "      <td>it's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.272222</td>\n",
       "      <td>-0.038889</td>\n",
       "      <td>7</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.267555</td>\n",
       "      <td>0.009556</td>\n",
       "      <td>28</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.256888</td>\n",
       "      <td>-0.128444</td>\n",
       "      <td>2</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.247777</td>\n",
       "      <td>-0.049555</td>\n",
       "      <td>5</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.236444</td>\n",
       "      <td>-0.016889</td>\n",
       "      <td>14</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.222444</td>\n",
       "      <td>-0.020222</td>\n",
       "      <td>11</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        score    weight  feature_value feature_name\n",
       "36   0.421332  0.052667              8           he\n",
       "149  0.351110  0.087778              4          see\n",
       "20   0.343999  0.038222              9          and\n",
       "25   0.291555  0.036444              8         it's\n",
       "70   0.272222 -0.038889              7         this\n",
       "10   0.267555  0.009556             28          the\n",
       "111  0.256888 -0.128444              2      nothing\n",
       "7    0.247777 -0.049555              5       action\n",
       "49   0.236444 -0.016889             14           to\n",
       "82   0.222444 -0.020222             11            a"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pick example that had wrong prediction:\n",
    "\n",
    "fel_example = bag_of_words(valid[376][:-1])\n",
    "y = valid[376][-1]\n",
    "\n",
    "#initialize empty lists\n",
    "score = []\n",
    "weight = []\n",
    "feature_value = []\n",
    "feature_name = []\n",
    "\n",
    "for f,v in fel_example.items():\n",
    "    score.append(np.abs(w.get(f,0)*v))\n",
    "    weight.append(w.get(f,0))\n",
    "    feature_value.append(v)\n",
    "    feature_name.append(f)\n",
    "    \n",
    "error_df = pd.DataFrame({'score':score, 'weight': weight, 'feature_value': feature_value, 'feature_name': feature_name})\n",
    "error_df.sort_values(by=['score'], inplace=True, ascending = False)\n",
    "error_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>weight</th>\n",
       "      <th>feature_value</th>\n",
       "      <th>feature_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.808887</td>\n",
       "      <td>-0.020222</td>\n",
       "      <td>40</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.802665</td>\n",
       "      <td>0.038222</td>\n",
       "      <td>21</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.554221</td>\n",
       "      <td>0.009556</td>\n",
       "      <td>58</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.345999</td>\n",
       "      <td>-0.038444</td>\n",
       "      <td>9</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.315999</td>\n",
       "      <td>-0.158000</td>\n",
       "      <td>2</td>\n",
       "      <td>only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.288888</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>13</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.263111</td>\n",
       "      <td>0.016444</td>\n",
       "      <td>16</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.249333</td>\n",
       "      <td>0.083111</td>\n",
       "      <td>3</td>\n",
       "      <td>brilliant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.230444</td>\n",
       "      <td>0.013556</td>\n",
       "      <td>17</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.212444</td>\n",
       "      <td>-0.053111</td>\n",
       "      <td>4</td>\n",
       "      <td>hero</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        score    weight  feature_value feature_name\n",
       "52   0.808887 -0.020222             40            a\n",
       "29   0.802665  0.038222             21          and\n",
       "0    0.554221  0.009556             58          the\n",
       "19   0.345999 -0.038444              9           on\n",
       "235  0.315999 -0.158000              2         only\n",
       "49   0.288888  0.022222             13         with\n",
       "68   0.263111  0.016444             16           is\n",
       "138  0.249333  0.083111              3    brilliant\n",
       "101  0.230444  0.013556             17         that\n",
       "191  0.212444 -0.053111              4         hero"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pick example that had wrong prediction:\n",
    "\n",
    "fel_example = bag_of_words(valid[35][:-1])\n",
    "y = valid[35][-1]\n",
    "\n",
    "#initialize empty lists\n",
    "score = []\n",
    "weight = []\n",
    "feature_value = []\n",
    "feature_name = []\n",
    "\n",
    "for f,v in fel_example.items():\n",
    "    score.append(np.abs(w.get(f,0)*v))\n",
    "    weight.append(w.get(f,0))\n",
    "    feature_value.append(v)\n",
    "    feature_name.append(f)\n",
    "    \n",
    "error_df = pd.DataFrame({'score':score, 'weight': weight, 'feature_value': feature_value, 'feature_name': feature_name})\n",
    "error_df.sort_values(by=['score'], inplace=True, ascending = False)\n",
    "error_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the features that have the highest score are associated with features that are repeated a lot, such as: 'and', 'the', 'of', 'is', 'in'. They are articles and prepositions and are not very informative.  \n",
    "There are lists of stopwords in english that can be used to remove these words from the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "- Construct new features to improve the performance\n",
    "- Use new loss function with standard error to evaluate statistical significance of improvements achieved by using new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "def folder_list(path,label):\n",
    "    '''\n",
    "    PARAMETER PATH IS THE PATH OF YOUR LOCAL FOLDER\n",
    "    '''\n",
    "    filelist = os.listdir(path)\n",
    "    review = []\n",
    "    for infile in filelist:\n",
    "        file = os.path.join(path,infile)\n",
    "        r = read_data(file)\n",
    "        r.append(label)\n",
    "        review.append(r)\n",
    "    return review\n",
    "\n",
    "\n",
    "def read_data(file):\n",
    "    '''\n",
    "    Read each file into a list of strings. Remove stopwords.\n",
    "    Example:\n",
    "    [\"it's\", 'a', 'curious', 'thing', \"i've\", 'found', 'that', 'when', 'willis', 'is', 'not', 'called', 'on', \n",
    "    ...'to', 'carry', 'the', 'whole', 'movie', \"he's\", 'much', 'better', 'and', 'so', 'is', 'the', 'movie']\n",
    "    'the' 'a' etc will be removed.\n",
    "    '''\n",
    "    f = open(file)\n",
    "    lines = f.read().split(' ') #split each line on whitespace\n",
    "    \n",
    "    symbols = '${}()[].,:;+-*/&|<>=~\" '\n",
    "    words = map(lambda Element: Element.translate(str.maketrans(\"\", \"\", symbols)).strip(), lines)\n",
    "    words = filter(None, words)\n",
    "    list_words = list(words)\n",
    "    \n",
    "    #Filter using a stopword list\n",
    "    sw = stopwords.words(\"english\")\n",
    "    filtered_words = [word for word in list_words if word not in sw]\n",
    "    \n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "def shuffle_data_v2():\n",
    "    '''\n",
    "    pos_path is where you save positive review data.\n",
    "    neg_path is where you save negative review data.\n",
    "    '''\n",
    "    pos_path = '/Users/Coline/Desktop/mlprojects/Bloomberg_foundations_of_ML/hw3-svm/data/pos'\n",
    "    neg_path = '/Users/Coline/Desktop/mlprojects/Bloomberg_foundations_of_ML/hw3-svm/data/neg'\n",
    "\n",
    "    #pos_path = r\"H:\\Ny mapp\\Literature\\Bayesian prob\\Bloomsberg lectures\\Week8\\hw3\\data\\data\\pos\"\n",
    "    #neg_path = r\"H:\\Ny mapp\\Literature\\Bayesian prob\\Bloomsberg lectures\\Week8\\hw3\\data\\data\\neg\"\n",
    "    \n",
    "    pos_review = folder_list(pos_path,1)\n",
    "    neg_review = folder_list(neg_path,-1)\n",
    "\n",
    "    review = pos_review + neg_review\n",
    "    random.shuffle(review)\n",
    "  \n",
    "    train = review[:1500]\n",
    "    valid = review[1500:]\n",
    "    \n",
    "    with open('train_filter.pkl', 'wb') as f:\n",
    "        pickle.dump(train, f)\n",
    "    \n",
    "    with open('valid_filter.pkl', 'wb') as f:\n",
    "        pickle.dump(valid, f)\n",
    "    \n",
    "    return train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = shuffle_data_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load files from memory\n",
    "with open('train_filter.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open('valid_filter.pkl', 'rb') as f:\n",
    "    valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "The model classified 79.0% of the examples correctly\n",
      "converged\n"
     ]
    }
   ],
   "source": [
    "w_filter = pegasos_v2(train, valid, epoch=250, l_reg=1e-2, tol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.6"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(valid, w_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using stopwords filter improved the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11,\n",
       " 15,\n",
       " 24,\n",
       " 27,\n",
       " 28,\n",
       " 38,\n",
       " 40,\n",
       " 43,\n",
       " 46,\n",
       " 50,\n",
       " 53,\n",
       " 63,\n",
       " 69,\n",
       " 73,\n",
       " 78,\n",
       " 86,\n",
       " 94,\n",
       " 98,\n",
       " 107,\n",
       " 113]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_idx = []\n",
    "for i in range(len(valid)):\n",
    "    y_i = valid[i][-1]\n",
    "    x_i = bag_of_words(valid[i][:-1])\n",
    "    pred = dotProduct(w_filter,x_i)\n",
    "    if pred/y_i < 0:\n",
    "        list_idx.append(i)\n",
    "list_idx[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>weight</th>\n",
       "      <th>feature_value</th>\n",
       "      <th>feature_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.270704</td>\n",
       "      <td>-0.135352</td>\n",
       "      <td>2</td>\n",
       "      <td>reason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.209089</td>\n",
       "      <td>-0.069696</td>\n",
       "      <td>3</td>\n",
       "      <td>would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.206059</td>\n",
       "      <td>-0.051515</td>\n",
       "      <td>4</td>\n",
       "      <td>save</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.205048</td>\n",
       "      <td>0.029293</td>\n",
       "      <td>7</td>\n",
       "      <td>keaton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.183837</td>\n",
       "      <td>0.091918</td>\n",
       "      <td>2</td>\n",
       "      <td>seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.175756</td>\n",
       "      <td>0.058585</td>\n",
       "      <td>3</td>\n",
       "      <td>movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.160604</td>\n",
       "      <td>-0.053535</td>\n",
       "      <td>3</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.153534</td>\n",
       "      <td>-0.076767</td>\n",
       "      <td>2</td>\n",
       "      <td>could</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.151514</td>\n",
       "      <td>0.050505</td>\n",
       "      <td>3</td>\n",
       "      <td>many</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.149493</td>\n",
       "      <td>-0.074747</td>\n",
       "      <td>2</td>\n",
       "      <td>even</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        score    weight  feature_value feature_name\n",
       "11   0.270704 -0.135352              2       reason\n",
       "105  0.209089 -0.069696              3        would\n",
       "34   0.206059 -0.051515              4         save\n",
       "27   0.205048  0.029293              7       keaton\n",
       "107  0.183837  0.091918              2         seen\n",
       "52   0.175756  0.058585              3       movies\n",
       "55   0.160604 -0.053535              3         kill\n",
       "50   0.153534 -0.076767              2        could\n",
       "79   0.151514  0.050505              3         many\n",
       "151  0.149493 -0.074747              2         even"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pick example that had wrong prediction:\n",
    "\n",
    "fel_example = bag_of_words(valid[113][:-1])\n",
    "y = valid[113][-1]\n",
    "\n",
    "#initialize empty lists\n",
    "score = []\n",
    "weight = []\n",
    "feature_value = []\n",
    "feature_name = []\n",
    "\n",
    "for f,v in fel_example.items():\n",
    "    score.append(np.abs(w_filter.get(f,0)*v))\n",
    "    weight.append(w_filter.get(f,0))\n",
    "    feature_value.append(v)\n",
    "    feature_name.append(f)\n",
    "    \n",
    "error_df = pd.DataFrame({'score':score, 'weight': weight, 'feature_value': feature_value, 'feature_name': feature_name})\n",
    "error_df.sort_values(by=['score'], inplace=True, ascending = False)\n",
    "error_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function with standard error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_error(p1, n):\n",
    "    p = np.sqrt(p1*(1-p1)/n)\n",
    "    return p\n",
    "\n",
    "def loss_std(valid, w):\n",
    "    '''\n",
    "    function that takes a sparse weight vector w and a collection of (x,y) pairs,\n",
    "    and returns the percent error when predicting y using sign(w^Tx) and the standard error. \n",
    "    \n",
    "    '''\n",
    "    count = 0\n",
    "    for i in range(len(valid)):\n",
    "        y_i = valid[i][-1]\n",
    "        x_i = bag_of_words(valid[i][:-1])\n",
    "        pred = dotProduct(w,x_i)\n",
    "        \n",
    "        if (pred/y_i) > 0: #occurs when y_i and w^Tx_i have the same sign (ie correct prediction)\n",
    "            count += 1\n",
    "            \n",
    "    percent = (count/(len(valid)))\n",
    "    \n",
    "    std = standard_error(percent, len(valid))\n",
    "    \n",
    "    return print('The percent error is {0:.2f} % with a standard error of +/- {1:.2f} %'.format(percent*100, std*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percent error is 84.60 % with a standard error of +/- 1.61 %\n"
     ]
    }
   ],
   "source": [
    "loss_std(valid, w_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other ideas for new features:\n",
    "\n",
    "- create new feature when a word is preceded by not (ex: 'not', 'good'  --> 'not good')\n",
    "- add/remove special characters\n",
    "- construct a list of negative words\n",
    "- implement TF-IDF instead of raw word counts to increase the weight of rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.837px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
