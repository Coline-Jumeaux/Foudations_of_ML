{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\">Introduction</a></span></li><li><span><a href=\"#Loading-the-data\" data-toc-modified-id=\"Loading-the-data-2\">Loading the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-all-the-data-and-randomly-split-it-into-1500-training-examples-and-500-validation-examples.\" data-toc-modified-id=\"Load-all-the-data-and-randomly-split-it-into-1500-training-examples-and-500-validation-examples.-2.1\">Load all the data and randomly split it into 1500 training examples and 500 validation examples.</a></span></li></ul></li><li><span><a href=\"#Sparse-Representations\" data-toc-modified-id=\"Sparse-Representations-3\">Sparse Representations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Function-that-converts-an-example-(e.g.-a-list-of-words)-into-a-sparse-bag-of-words-representation:\" data-toc-modified-id=\"Function-that-converts-an-example-(e.g.-a-list-of-words)-into-a-sparse-bag-of-words-representation:-3.1\">Function that converts an example (e.g. a list of words) into a sparse bag-of-words representation:</a></span></li></ul></li><li><span><a href=\"#Support-Vector-Machine-via-Pegasos\" data-toc-modified-id=\"Support-Vector-Machine-via-Pegasos-4\">Support Vector Machine via Pegasos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pegasos-algorithm\" data-toc-modified-id=\"Pegasos-algorithm-4.1\">Pegasos algorithm</a></span></li><li><span><a href=\"#Support-Vector-Machine-(SVM)\" data-toc-modified-id=\"Support-Vector-Machine-(SVM)-4.2\">Support Vector Machine (SVM)</a></span></li><li><span><a href=\"#Implementation-of-the-Pegasos-algorithm-to-run-on-a-sparse-data-representation\" data-toc-modified-id=\"Implementation-of-the-Pegasos-algorithm-to-run-on-a-sparse-data-representation-4.3\">Implementation of the Pegasos algorithm to run on a sparse data representation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Version-1\" data-toc-modified-id=\"Version-1-4.3.1\">Version 1</a></span></li><li><span><a href=\"#Version-2\" data-toc-modified-id=\"Version-2-4.3.2\">Version 2</a></span></li></ul></li><li><span><a href=\"#Comparison-of-the-two-approaches\" data-toc-modified-id=\"Comparison-of-the-two-approaches-4.4\">Comparison of the two approaches</a></span></li><li><span><a href=\"#Loss-function\" data-toc-modified-id=\"Loss-function-4.5\">Loss function</a></span></li></ul></li><li><span><a href=\"#Regularization-parameter-optimization\" data-toc-modified-id=\"Regularization-parameter-optimization-5\">Regularization parameter optimization</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: SVM and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment for the [Foundations of Machine Learning](https://bloomberg.github.io/foml/#about) course.\n",
    "The assignment text can be found in the SVM folder under hw3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Goal of the assignment: perform sentiment analysis on movie reviews. We will be working with **natural language data**. \n",
    "- First, the **Pegasos algorithm** is implemented. Pegasos is essentially stochastic subgradient descent for the SVM with a particular schedule for the step-size.\n",
    "- Second, because in natural language domains we typically have huge feature spaces, we work with **sparse representations** of feature vectors, where only the non-zero entries are explicitly recorded. This will require coding the gradient and SGD code using hash tables (dictionaries in Python), rather than numpy arrays.\n",
    "- Then we will perform feature engineering to improve our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will be using the [Polarity Dataset v2.0](https://www.cs.cornell.edu/people/pabo/movie-review-data/), constructed by Pang and Lee. \n",
    "- It has the full text from 2000 movies reviews: 1000 reviews are classified as “positive” and 1000 as “negative”. Our goal is to predict whether a review has positive or negative sentiment from the text of the review. \n",
    "- Each review is stored in a separate file: the positive reviews are in a folder called “pos”, and the negative reviews are in “neg”. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all the data and randomly split it into 1500 training examples and 500 validation examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################Code provided in the assignment###################################################\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "'''\n",
    "Note:  This code is just a hint for people who are not familiar with text processing in python. \n",
    "There is no obligation to use this code, though you may if you like. \n",
    "'''\n",
    "\n",
    "\n",
    "def folder_list(path,label):\n",
    "    '''\n",
    "    PARAMETER PATH IS THE PATH OF YOUR LOCAL FOLDER\n",
    "    '''\n",
    "    filelist = os.listdir(path)\n",
    "    review = []\n",
    "    for infile in filelist:\n",
    "        file = os.path.join(path,infile)\n",
    "        r = read_data(file)\n",
    "        r.append(label)\n",
    "        review.append(r)\n",
    "    return review\n",
    "\n",
    "def read_data(file):\n",
    "    '''\n",
    "    Read each file into a list of strings. \n",
    "    Example:\n",
    "    [\"it's\", 'a', 'curious', 'thing', \"i've\", 'found', 'that', 'when', 'willis', 'is', 'not', 'called', 'on', \n",
    "    ...'to', 'carry', 'the', 'whole', 'movie', \"he's\", 'much', 'better', 'and', 'so', 'is', 'the', 'movie']\n",
    "    '''\n",
    "    f = open(file)\n",
    "    lines = f.read().split(' ') #split each line on whitespace\n",
    "    symbols = '${}()[].,:;+-*/&|<>=~\" '\n",
    "    words = map(lambda Element: Element.translate(str.maketrans(\"\", \"\", symbols)).strip(), lines)\n",
    "    words = filter(None, words)\n",
    "    list_words = list(words)\n",
    "    return list_words\n",
    "\n",
    "###############################################\n",
    "######## YOUR CODE STARTS FROM HERE. ##########\n",
    "###############################################\n",
    "\n",
    "def shuffle_data():\n",
    "    '''\n",
    "    pos_path is where you save positive review data.\n",
    "    neg_path is where you save negative review data.\n",
    "    '''\n",
    "    pos_path = '/Users/Coline/Desktop/mlprojects/Bloomberg_foundations_of_ML/hw3-svm/data/pos'\n",
    "    neg_path = '/Users/Coline/Desktop/mlprojects/Bloomberg_foundations_of_ML/hw3-svm/data/neg'\n",
    "    \n",
    "    pos_review = folder_list(pos_path,1)\n",
    "    neg_review = folder_list(neg_path,-1)\n",
    "\n",
    "    review = pos_review + neg_review\n",
    "    random.shuffle(review)\n",
    "  \n",
    "    '''\n",
    "    Now you have read all the files into list 'review' and it has been shuffled.\n",
    "    Save your shuffled result by pickle.\n",
    "    *Pickle is a useful module to serialize a python object structure. \n",
    "    *Check it out. https://wiki.python.org/moin/UsingPickle\n",
    "    '''\n",
    "    train = review[:1500]\n",
    "    valid = review[1500:]\n",
    "    \n",
    "    with open('train.pkl', 'wb') as f:\n",
    "        pickle.dump(train, f)\n",
    "    \n",
    "    with open('valid.pkl', 'wb') as f:\n",
    "        pickle.dump(valid, f)\n",
    "    \n",
    "    return train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = shuffle_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load files from memory\n",
    "with open('train.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open('valid.pkl', 'rb') as f:\n",
    "    valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The most basic way to represent text documents for machine learning is with a **“bag-of-words”** representation. \n",
    "- Here every possible word is a feature, and the value of a word feature is the number of times that word appears in the document. Of course, most words will not appear in any particular document, and those counts will be zero. \n",
    "- Rather than store a huge number of zeros, we use a **sparse representation**, in which we only store the counts that are nonzero. The counts are stored in a key/value store (such as a dictionary in Python).  \n",
    "For example, “Harry Potter and Harry Potter II” would be represented as the following Python dict: ``x={’Harry’:2, ’Potter’:2, ’and’:1, ’II’:1}``.  \n",
    "- We will be using linear classifiers of the form $f(x) = w^T x$, and we can store the $w$ vector in a sparse format as well, such as ``w={’minimal’:1.3, ’Harry’:-1.1, ’viable’:-4.2, ’and’:2.2, ’product’:9.1}``. \n",
    "- The inner product between $w$ and $x$ would only involve the features that appear in both $x$ and $w$, since whatever doesn’t appear is assumed to be zero. For this example, the inner product would be ``x[Harry] * w[Harry] + x[and] * w[and] = 2*(-1.1) + 1*(2.2)``.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that converts an example (e.g. a list of words) into a sparse bag-of-words representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def bag_of_words(review):\n",
    "    '''\n",
    "    Converts a list of words into a sparse bag-of-words representation.\n",
    "    @Param review: list (iterable)\n",
    "    Returns a dictionary with keys=words and values=number of times the words appear\n",
    "    '''\n",
    "    cnt = Counter()\n",
    "    for word in review:\n",
    "        cnt[word] += 1\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine via Pegasos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question you will build an SVM using the Pegasos algorithm. To align with the notation used in the [Pegasos paper](https://ttic.uchicago.edu/~nati/Publications/PegasosMPB.pdf), we’re considering the following formulation of the SVM objective function: $$ min_{w \\in R^d} \\frac{\\lambda}{2} + \\frac{1}{m} \\sum_{i=1}^m max\\{0, 1-y_iw^Tx_i\\} $$\n",
    "\n",
    "Note that, for simplicity, we are leaving off the unregularized bias term $b$. \n",
    "\n",
    "### Pegasos algorithm\n",
    "Pegasos is stochastic subgradient descent using a step size rule $\\eta_t = \\frac{1}{\\lambda t}$. The pseudocode is given below:\n",
    "\n",
    "***\n",
    "Input: $\\lambda > 0$. Choose $w_1 = 0; t = 0$\n",
    "\n",
    "While termination condition not met  \n",
    "\n",
    "&nbsp;&nbsp;For $j = 1,...,m$ (assumes data is randomly permuted)  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$t = t + 1$  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$\\eta_t = \\frac{1}{t\\lambda}$;  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;If $y_j w^T_t x_j < 1$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$w_{t+1} = (1 - \\eta_t \\lambda)w_t + \\eta_t y_j x_j$  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Else  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$w_{t+1} = (1 - \\eta_t \\lambda)w_t$ \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "The “stochastic” SVM objective function is the SVM objective function with a single training point:  \n",
    "  \n",
    "$$J_i(w) = \\frac{\\lambda}{2} \\|w\\|^2 + max\\{0, 1-y_iw^Tx_i\\}$$   \n",
    "The function  $J_i(w)$ is not differentiable at $y_i w^T x_i = 1$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subgradient of $J_i(w)$ is given by \n",
    "\n",
    "$$ g =\n",
    "  \\begin{cases}\n",
    "    \\lambda w - y_i x_i       & \\quad \\text{for } y_i w^T x_i < 1 \\\\\n",
    "    \\lambda w  & \\quad \\text{for } y_i w^T x_i \\geq 1\n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the step size rule is $\\eta_t = \\frac{1}{\\lambda t}$, then doing SGD with the subgradient direction from the previous problem is the same as given in the pseudocode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the Pegasos algorithm to run on a sparse data representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################code given in the assignment#############################################\n",
    "# Taken from http://web.stanford.edu/class/cs221/ Assignment #2 Support Code\n",
    "\n",
    "def dotProduct(d1, d2):\n",
    "    \"\"\"\n",
    "    @param dict d1: a feature vector represented by a mapping from a feature (string) to a weight (float).\n",
    "    @param dict d2: same as d1\n",
    "    @return float: the dot product between d1 and d2\n",
    "    \"\"\"\n",
    "    if len(d1) < len(d2):\n",
    "        return dotProduct(d2, d1)\n",
    "    else:\n",
    "        return sum(d1.get(f, 0) * v for f, v in d2.items())\n",
    "\n",
    "def increment(d1, scale, d2):\n",
    "    \"\"\"\n",
    "    Implements d1 += scale * d2 for sparse vectors.\n",
    "    @param dict d1: the feature vector which is mutated.\n",
    "    @param float scale\n",
    "    @param dict d2: a feature vector.\n",
    "\n",
    "    NOTE: This function does not return anything, but rather\n",
    "    increments d1 in place. We do this because it is much faster to\n",
    "    change elements of d1 in place than to build a new dictionary and\n",
    "    return it.\n",
    "    \"\"\"\n",
    "    for f, v in d2.items():\n",
    "        d1[f] = d1.get(f, 0) + v * scale\n",
    "        \n",
    "#########################################end of code given in the assignment#####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pegasos(train, epoch, l_reg=0.1):\n",
    "    '''\n",
    "    Implements Pegasos algorithm running on sparse data representation. \n",
    "    @param train: list of lists, each list contains single words from the review\n",
    "    @param epoch: number of epochs, integer\n",
    "    @param l_reg: regularization parameter lambda, float\n",
    "    \n",
    "    The weight dictionnary w is initialized as an empty dictionnary. \n",
    "    For each epoch, the algorithm passes through each datapoint of the randomized dataset and the step size \n",
    "    is decreased by a factor of 1/t (t increments after each datapoint).\n",
    "    Each datapoint is transformed into a bag of words using the bag_of_words function, and the label y_j is \n",
    "    extracted. The gradient is calculated using the appropriate subgradient, defined by the value of the\n",
    "    margin (calculated using the dotProduct function). The weight dictionnary is updated with the gradient\n",
    "    and step size using the increment function.\n",
    "    '''\n",
    "    w = {}\n",
    "    t = 0\n",
    "\n",
    "    for i in range(epoch):\n",
    "        \n",
    "        if i%10 == 0:\n",
    "            print(\"Epoch: {}\".format(i))\n",
    "        \n",
    "        for j in range(len(train)):\n",
    "            t = t+1\n",
    "            step = 1/(l_reg*t)\n",
    "            x_j = bag_of_words(train[j][:-1])\n",
    "            y_j = train[j][-1]                   #The label is the last entry in the list\n",
    "            margin = y_j*dotProduct(w,x_j)\n",
    "        \n",
    "            if margin < 1:\n",
    "                increment(w, -step*l_reg, w)\n",
    "                increment(w, step*y_j, x_j)\n",
    "            \n",
    "            else:\n",
    "                increment(w, -step*l_reg, w)\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    }
   ],
   "source": [
    "w1 = pegasos(train, epoch=2, l_reg=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that in every step of the Pegasos algorithm, we rescale every entry of $w_t$ by the factor $(1 − \\eta_t \\lambda)$. \n",
    "- Implementing this directly with dictionaries is very slow. \n",
    "- We can make things significantly faster by representing $w$ as $w = sW$, where $s \\in R$ and $W \\in R^d$. \n",
    "- You can start with $s = 1$ and $W$ all zeros (i.e. an empty dictionary). Note that both updates (i.e. whether or not we have a margin error) start with rescaling $w_t$, which we can do simply by setting $s_{t+1} = (1 − \\eta_t \\lambda)s_t$. If the update is $w_{t+1} = (1 − \\eta_t \\lambda)w_t + \\eta_t y_j x_j$, then **verify that the Pegasos update step is equivalent to**:  \n",
    "\n",
    "$$s_{t+1} = (1 − \\eta_t \\lambda)s_t$$\n",
    "$$W_{t+1} = W_t + \\frac{1}{s_{t+1}} \\eta_t y_j x_j.$$\n",
    "\n",
    "There is one subtle issue with the approach described above: if we ever have $1 − \\eta_t \\lambda = 0$, then $s_{t+1} = 0$, and we’ll have a divide by 0 in the calculation for $W_{t+1}$. This only happens when $\\eta_t = 1/\\lambda$. With our step-size rule of $\\eta_t = 1/\\lambda t$, it happens exactly when $t = 1$. So one approach is to just start at $t = 2$. More generically, note that if $s_{t+1} = 0$, then $w_{t+1} = 0$. Thus an equivalent representation is $s_{t+1} = 1$ and $W = 0$. Thus if we ever get $s_{t+1} = 0$, simply set it back to 1 and reset $W_{t+1}$ to zero, which is an empty dictionary in a sparse representation.  \n",
    "Implement the Pegasos algorithm with the $(s, W)$ representation described above. [See section 5.1 of Leon Bottou’s Stochastic Gradient Tricks for a more generic version of this technique, and many other useful tricks.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pegasos_v2(train, valid, epoch, l_reg=0.1, tol=1e-9):\n",
    "    '''\n",
    "    pegasos_v2 function implements Pegasos algorithm running on sparse data representation. \n",
    "    @param train: list of lists, each list contains single words from the review. Training set\n",
    "    @param valid: list of lists, each list contains single words from the review. Validation set\n",
    "    @param epoch: number of epochs, integer\n",
    "    @param l_reg: regularization parameter lambda, float\n",
    "    @param tol: tolerance level for estimation of convergence, float\n",
    "    \n",
    "    The weight dictionnary w is represented as w=sW, where s in is R and W is in Rd. \n",
    "    The parameters initialization is s=1 and W empty dictionary.\n",
    "    '''\n",
    "    s = 1\n",
    "    W = {}\n",
    "    t = 1\n",
    "    w = {}\n",
    "\n",
    "    for i in range(epoch):\n",
    "         \n",
    "        #Storing the weight vector to perform estimation of convergence after each epoch\n",
    "        w_old = W.copy() \n",
    "        for a,b in w_old.items():\n",
    "            w_old[a] = b*s_new\n",
    "\n",
    "        for j in range(len(train)):\n",
    "            t = t+1\n",
    "            step = 1/(l_reg*t)\n",
    "            x_j = bag_of_words(train[j][:-1])\n",
    "            y_j = train[j][-1]\n",
    "             \n",
    "            margin = s*y_j*dotProduct(W,x_j)\n",
    "            \n",
    "            if margin < 1:\n",
    "                s_new = s*(1-step*l_reg)\n",
    "                increment(W, (step*y_j)/s_new, x_j)\n",
    "                s = s_new\n",
    "            \n",
    "            else:\n",
    "                s_new = s*(1-step*l_reg)\n",
    "                s = s_new    \n",
    "            \n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print(\"Epoch: {}\".format(i))\n",
    "            w = W.copy()\n",
    "            for a,b in w.items():\n",
    "                w[a] = b*s_new\n",
    "            loss_val = loss(valid,w)\n",
    "            print('The model classified {}% of the examples correctly'.format(loss_val))\n",
    "        \n",
    "        #Testing for convergence:\n",
    "        if i != 0: \n",
    "            list_weights_diff = []\n",
    "            w = W.copy()\n",
    "            for a,b in w.items():\n",
    "                w[a] = b*s_new\n",
    "            for k, v in w_old.items():\n",
    "                list_weights_diff.append(np.abs(w.get(k, 0) - w_old[k])) \n",
    "        \n",
    "            if np.all(np.asarray(list_weights_diff) < tol):\n",
    "                print('converged')\n",
    "                break   \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the two approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "CPU times: user 23.1 s, sys: 184 ms, total: 23.3 s\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%time w1 = pegasos(train, epoch=2, l_reg=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-7118e33fa609>\u001b[0m in \u001b[0;36mpegasos_v2\u001b[0;34m(train, valid, epoch, l_reg, tol)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The model classified {}% of the examples correctly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "%time w2 = pegasos_v2(train, valid, epoch=2, l_reg=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2e10b6aa8056>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "loss(valid, w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(valid, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the content of the weight dicts:\n",
    "list_weights_diff = []\n",
    "for k, v in w1.items():\n",
    "    list_weights_diff.append(np.abs(w2.get(k, 0) - w1[k]))\n",
    "np.all(np.asarray(list_weights_diff) < 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When expressing the vector $w_t$ as the product $s_tW_t$, where s is a scalar, as was done in pegasos_v2 algorithm, the algorithm is sped up by a factor of 14, which is significantly faster.\n",
    "- The performance of the algorithms was equivalent: they had the same classification error on the training set, and the weight vectors were very similar.\n",
    "- In the rest of this assignement, I will use pegasos_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(valid, w):\n",
    "    '''\n",
    "    function that takes a sparse weight vector w and a collection of (x,y) pairs,\n",
    "    and returns the percent error when predicting y using sign(w^Tx). \n",
    "    In other words, the function reports the 0-1 loss of the linear predictor x -> w^Tx.\n",
    "    '''\n",
    "    count = 0\n",
    "    for i in range(len(valid)):\n",
    "        y_i = valid[i][-1]\n",
    "        x_i = bag_of_words(valid[i][:-1])\n",
    "        pred = dotProduct(w,x_i)\n",
    "        \n",
    "        if (pred/y_i) > 0: #occurs when y_i and w^Tx_i have the same sign (ie correct prediction)\n",
    "            count += 1\n",
    "            \n",
    "    percent = (count/(len(valid)))*100\n",
    "    \n",
    "    return percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization parameter optimization\n",
    "\n",
    "- Using the bag-of-words feature representation described above, search for the regularization parameter that gives the minimal percent error on your test set. \n",
    "- Using the faster Pegasos implementation, running to convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Range of lambda values\n",
    "l_reg_arr = np.unique(np.concatenate((10.**np.arange(-6,1,1), np.arange(1,3,.3))))\n",
    "\n",
    "#Initialize values\n",
    "w_history = []\n",
    "loss_history = []\n",
    "\n",
    "#Run SVM algo for each lambda\n",
    "for l_reg in l_reg_arr:\n",
    "    w = pegasos_v2(train, valid, epoch=250, l_reg=l_reg, tol=1e-2)\n",
    "    w_history.append(w)\n",
    "    \n",
    "    #Calculate misclassification percentage for each lambda\n",
    "    loss_val = 100-loss(valid, w)\n",
    "    loss_history.append(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Present the results in a table\n",
    "import pandas as pd\n",
    "df_error = pd.DataFrame({'Param_l_reg': l_reg_arr, 'Classification error': loss_history})\n",
    "\n",
    "print(df_error)\n",
    "\n",
    "min_idx = df_error['Classification error'].idxmin()\n",
    "print('The regression parameter that minimizes test loss was {:.2e}.'.format(df_error['Param_l_reg'][min_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot validation performance vs regularization parameter\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.semilogx(df_error[\"Param_l_reg\"], df_error[\"Classification error\"])\n",
    "ax.grid()\n",
    "ax.set_title(\"Test Performance vs Regularization\")\n",
    "ax.set_xlabel(\"L-Penalty Regularization Parameter\")\n",
    "ax.set_ylabel(\"Classification error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, the regularization parameters in the range [1e-4,1e-2] achieved the best performance for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.837px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
